{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5466u_0uHAZ"
      },
      "source": [
        "# Homework 5 (Full mark: 100pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPA6M4W-uHAd"
      },
      "source": [
        "You can use ``Google Colab`` if you would like to use GPU.\n",
        "- https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "- https://theorydb.github.io/dev/2019/08/23/dev-ml-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoE4JgR4uHAe"
      },
      "source": [
        "# 1. Regression (50pt)\n",
        "\n",
        "**For this question, using PyTorch, implement the 1) ridge regression and 2) Lasso. You can refer to the tutorial link below for how to implement linear regression. Note that the ridge regression is the linear regression with L2 penalty, and the Lasso is the linear regression with L1 penalty. You should use ````Boston```` dataset as shown in the code below. You should not only write the code for the models, but also train them and show the test MSE.**\n",
        "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/linear_regression/main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtewhiE1uHAe"
      },
      "source": [
        "<div>\n",
        "<img src=\"figures/regressions.png\" width=\"700\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o_zCFMy25sQ",
        "outputId": "438f8985-1dbd-4972-977c-1a61276e4c0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR='/content/drive/MyDrive/ML IE/HW5'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/ML IE/HW5'"
      ],
      "metadata": {
        "id": "D_Ri2_wlXUXn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dSmNWcT0uHAf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "# To fix the random seed\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# load data\n",
        "#boston = pd.read_csv('data/Boston.csv').drop('Unnamed: 0', axis=1)\n",
        "boston = pd.read_csv('/content/drive/MyDrive/ML IE/HW5/data/Boston.csv').drop('Unnamed: 0', axis=1)\n",
        "data = torch.FloatTensor(boston.values)\n",
        "X = data[:,:-1] # Input (X)\n",
        "y = data[:,-1].reshape(-1, 1) # Ground Truth (y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000          #100    I change num_epochs and lr for effective train\n",
        "learning_rate = 0.000001   #0.0003\n",
        "torch.manual_seed(3)\n",
        "\n",
        "class Ridge(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        ## Write your answer here\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = 1\n",
        "        self.fc1 = nn.Linear(self.input_dim,self.output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        ## Write your answer here\n",
        "        y1 = self.fc1(X)\n",
        "\n",
        "        return y1\n",
        "\n",
        "class Lasso(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        ## Write your answer here\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = 1\n",
        "        self.fc1 = nn.Linear(self.input_dim,self.output_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        ## Write your answer here\n",
        "        y1 = self.fc1(X)\n",
        "        return y1\n",
        "\n",
        "## Write your answer here (Training code)\n",
        "def train(model, X, y, epochs, c = 1, penalty= 'l1'):\n",
        "\n",
        "    # regularity\n",
        "    c = torch.FloatTensor([c])\n",
        "    p = 1 if penalty == 'l1' else 2\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        inputs = X\n",
        "        targets = y\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        #print(p)\n",
        "        loss = criterion(outputs, targets) + c * torch.norm(model.fc1.weight, p =p)  # total loss = mse + penalty(L1 or L2)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "\n",
        "## Train Ridge and Lasso and show the final test MSE.\n",
        "input_dim = X.size()[1]  #13 features\n",
        "\n",
        "model_r = Ridge(input_dim)\n",
        "model_l = Lasso(input_dim)\n",
        "model1 = train(model_l, X_train, y_train, epochs = num_epochs, c=1, penalty= 'l2')\n",
        "model2 = train(model_r, X_train, y_train, epochs = num_epochs, c=1, penalty= 'l1')\n",
        "\n",
        "\n",
        "y_hat_r = model_r(X_test).detach().numpy()\n",
        "y_hat_l = model_l(X_test).detach().numpy()\n",
        "y_test_np = y_test.detach().numpy()\n",
        "y_hat_l = model_l(X_test).detach().numpy()\n",
        "final_MSE_r = np.mean((y_hat_r - y_test_np)**2)\n",
        "final_MSE_l = np.mean((y_hat_l - y_test_np)**2)\n",
        "print('final_MSE for ridge:',final_MSE_r)\n",
        "print('final_MSE for lasso:',final_MSE_l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGT2uqZUqwTj",
        "outputId": "6d5144c2-3eb7-4e43-d314-8a328ef5e50f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/1000], Loss: 104.7677\n",
            "Epoch [10/1000], Loss: 85.4990\n",
            "Epoch [15/1000], Loss: 84.3764\n",
            "Epoch [20/1000], Loss: 83.5967\n",
            "Epoch [25/1000], Loss: 83.0311\n",
            "Epoch [30/1000], Loss: 82.5997\n",
            "Epoch [35/1000], Loss: 82.2531\n",
            "Epoch [40/1000], Loss: 81.9605\n",
            "Epoch [45/1000], Loss: 81.7027\n",
            "Epoch [50/1000], Loss: 81.4676\n",
            "Epoch [55/1000], Loss: 81.2478\n",
            "Epoch [60/1000], Loss: 81.0388\n",
            "Epoch [65/1000], Loss: 80.8374\n",
            "Epoch [70/1000], Loss: 80.6421\n",
            "Epoch [75/1000], Loss: 80.4515\n",
            "Epoch [80/1000], Loss: 80.2649\n",
            "Epoch [85/1000], Loss: 80.0818\n",
            "Epoch [90/1000], Loss: 79.9020\n",
            "Epoch [95/1000], Loss: 79.7252\n",
            "Epoch [100/1000], Loss: 79.5513\n",
            "Epoch [105/1000], Loss: 79.3802\n",
            "Epoch [110/1000], Loss: 79.2118\n",
            "Epoch [115/1000], Loss: 79.0459\n",
            "Epoch [120/1000], Loss: 78.8827\n",
            "Epoch [125/1000], Loss: 78.7219\n",
            "Epoch [130/1000], Loss: 78.5636\n",
            "Epoch [135/1000], Loss: 78.4077\n",
            "Epoch [140/1000], Loss: 78.2541\n",
            "Epoch [145/1000], Loss: 78.1029\n",
            "Epoch [150/1000], Loss: 77.9540\n",
            "Epoch [155/1000], Loss: 77.8074\n",
            "Epoch [160/1000], Loss: 77.6629\n",
            "Epoch [165/1000], Loss: 77.5206\n",
            "Epoch [170/1000], Loss: 77.3805\n",
            "Epoch [175/1000], Loss: 77.2425\n",
            "Epoch [180/1000], Loss: 77.1065\n",
            "Epoch [185/1000], Loss: 76.9726\n",
            "Epoch [190/1000], Loss: 76.8407\n",
            "Epoch [195/1000], Loss: 76.7108\n",
            "Epoch [200/1000], Loss: 76.5828\n",
            "Epoch [205/1000], Loss: 76.4567\n",
            "Epoch [210/1000], Loss: 76.3325\n",
            "Epoch [215/1000], Loss: 76.2101\n",
            "Epoch [220/1000], Loss: 76.0895\n",
            "Epoch [225/1000], Loss: 75.9707\n",
            "Epoch [230/1000], Loss: 75.8537\n",
            "Epoch [235/1000], Loss: 75.7384\n",
            "Epoch [240/1000], Loss: 75.6248\n",
            "Epoch [245/1000], Loss: 75.5128\n",
            "Epoch [250/1000], Loss: 75.4025\n",
            "Epoch [255/1000], Loss: 75.2938\n",
            "Epoch [260/1000], Loss: 75.1867\n",
            "Epoch [265/1000], Loss: 75.0812\n",
            "Epoch [270/1000], Loss: 74.9772\n",
            "Epoch [275/1000], Loss: 74.8747\n",
            "Epoch [280/1000], Loss: 74.7737\n",
            "Epoch [285/1000], Loss: 74.6741\n",
            "Epoch [290/1000], Loss: 74.5760\n",
            "Epoch [295/1000], Loss: 74.4793\n",
            "Epoch [300/1000], Loss: 74.3839\n",
            "Epoch [305/1000], Loss: 74.2900\n",
            "Epoch [310/1000], Loss: 74.1974\n",
            "Epoch [315/1000], Loss: 74.1061\n",
            "Epoch [320/1000], Loss: 74.0161\n",
            "Epoch [325/1000], Loss: 73.9274\n",
            "Epoch [330/1000], Loss: 73.8400\n",
            "Epoch [335/1000], Loss: 73.7538\n",
            "Epoch [340/1000], Loss: 73.6688\n",
            "Epoch [345/1000], Loss: 73.5850\n",
            "Epoch [350/1000], Loss: 73.5023\n",
            "Epoch [355/1000], Loss: 73.4209\n",
            "Epoch [360/1000], Loss: 73.3406\n",
            "Epoch [365/1000], Loss: 73.2614\n",
            "Epoch [370/1000], Loss: 73.1833\n",
            "Epoch [375/1000], Loss: 73.1063\n",
            "Epoch [380/1000], Loss: 73.0303\n",
            "Epoch [385/1000], Loss: 72.9554\n",
            "Epoch [390/1000], Loss: 72.8815\n",
            "Epoch [395/1000], Loss: 72.8087\n",
            "Epoch [400/1000], Loss: 72.7368\n",
            "Epoch [405/1000], Loss: 72.6660\n",
            "Epoch [410/1000], Loss: 72.5961\n",
            "Epoch [415/1000], Loss: 72.5271\n",
            "Epoch [420/1000], Loss: 72.4591\n",
            "Epoch [425/1000], Loss: 72.3920\n",
            "Epoch [430/1000], Loss: 72.3258\n",
            "Epoch [435/1000], Loss: 72.2605\n",
            "Epoch [440/1000], Loss: 72.1961\n",
            "Epoch [445/1000], Loss: 72.1325\n",
            "Epoch [450/1000], Loss: 72.0698\n",
            "Epoch [455/1000], Loss: 72.0079\n",
            "Epoch [460/1000], Loss: 71.9469\n",
            "Epoch [465/1000], Loss: 71.8866\n",
            "Epoch [470/1000], Loss: 71.8272\n",
            "Epoch [475/1000], Loss: 71.7685\n",
            "Epoch [480/1000], Loss: 71.7106\n",
            "Epoch [485/1000], Loss: 71.6534\n",
            "Epoch [490/1000], Loss: 71.5970\n",
            "Epoch [495/1000], Loss: 71.5413\n",
            "Epoch [500/1000], Loss: 71.4864\n",
            "Epoch [505/1000], Loss: 71.4321\n",
            "Epoch [510/1000], Loss: 71.3786\n",
            "Epoch [515/1000], Loss: 71.3257\n",
            "Epoch [520/1000], Loss: 71.2735\n",
            "Epoch [525/1000], Loss: 71.2220\n",
            "Epoch [530/1000], Loss: 71.1711\n",
            "Epoch [535/1000], Loss: 71.1209\n",
            "Epoch [540/1000], Loss: 71.0713\n",
            "Epoch [545/1000], Loss: 71.0223\n",
            "Epoch [550/1000], Loss: 70.9739\n",
            "Epoch [555/1000], Loss: 70.9262\n",
            "Epoch [560/1000], Loss: 70.8790\n",
            "Epoch [565/1000], Loss: 70.8324\n",
            "Epoch [570/1000], Loss: 70.7864\n",
            "Epoch [575/1000], Loss: 70.7410\n",
            "Epoch [580/1000], Loss: 70.6961\n",
            "Epoch [585/1000], Loss: 70.6517\n",
            "Epoch [590/1000], Loss: 70.6079\n",
            "Epoch [595/1000], Loss: 70.5646\n",
            "Epoch [600/1000], Loss: 70.5219\n",
            "Epoch [605/1000], Loss: 70.4796\n",
            "Epoch [610/1000], Loss: 70.4379\n",
            "Epoch [615/1000], Loss: 70.3967\n",
            "Epoch [620/1000], Loss: 70.3559\n",
            "Epoch [625/1000], Loss: 70.3156\n",
            "Epoch [630/1000], Loss: 70.2758\n",
            "Epoch [635/1000], Loss: 70.2365\n",
            "Epoch [640/1000], Loss: 70.1976\n",
            "Epoch [645/1000], Loss: 70.1592\n",
            "Epoch [650/1000], Loss: 70.1212\n",
            "Epoch [655/1000], Loss: 70.0836\n",
            "Epoch [660/1000], Loss: 70.0465\n",
            "Epoch [665/1000], Loss: 70.0098\n",
            "Epoch [670/1000], Loss: 69.9735\n",
            "Epoch [675/1000], Loss: 69.9377\n",
            "Epoch [680/1000], Loss: 69.9022\n",
            "Epoch [685/1000], Loss: 69.8671\n",
            "Epoch [690/1000], Loss: 69.8325\n",
            "Epoch [695/1000], Loss: 69.7982\n",
            "Epoch [700/1000], Loss: 69.7643\n",
            "Epoch [705/1000], Loss: 69.7307\n",
            "Epoch [710/1000], Loss: 69.6975\n",
            "Epoch [715/1000], Loss: 69.6647\n",
            "Epoch [720/1000], Loss: 69.6323\n",
            "Epoch [725/1000], Loss: 69.6002\n",
            "Epoch [730/1000], Loss: 69.5684\n",
            "Epoch [735/1000], Loss: 69.5370\n",
            "Epoch [740/1000], Loss: 69.5059\n",
            "Epoch [745/1000], Loss: 69.4751\n",
            "Epoch [750/1000], Loss: 69.4447\n",
            "Epoch [755/1000], Loss: 69.4146\n",
            "Epoch [760/1000], Loss: 69.3848\n",
            "Epoch [765/1000], Loss: 69.3553\n",
            "Epoch [770/1000], Loss: 69.3261\n",
            "Epoch [775/1000], Loss: 69.2972\n",
            "Epoch [780/1000], Loss: 69.2686\n",
            "Epoch [785/1000], Loss: 69.2402\n",
            "Epoch [790/1000], Loss: 69.2122\n",
            "Epoch [795/1000], Loss: 69.1845\n",
            "Epoch [800/1000], Loss: 69.1570\n",
            "Epoch [805/1000], Loss: 69.1298\n",
            "Epoch [810/1000], Loss: 69.1029\n",
            "Epoch [815/1000], Loss: 69.0762\n",
            "Epoch [820/1000], Loss: 69.0498\n",
            "Epoch [825/1000], Loss: 69.0236\n",
            "Epoch [830/1000], Loss: 68.9977\n",
            "Epoch [835/1000], Loss: 68.9721\n",
            "Epoch [840/1000], Loss: 68.9467\n",
            "Epoch [845/1000], Loss: 68.9215\n",
            "Epoch [850/1000], Loss: 68.8966\n",
            "Epoch [855/1000], Loss: 68.8719\n",
            "Epoch [860/1000], Loss: 68.8474\n",
            "Epoch [865/1000], Loss: 68.8232\n",
            "Epoch [870/1000], Loss: 68.7991\n",
            "Epoch [875/1000], Loss: 68.7753\n",
            "Epoch [880/1000], Loss: 68.7518\n",
            "Epoch [885/1000], Loss: 68.7284\n",
            "Epoch [890/1000], Loss: 68.7052\n",
            "Epoch [895/1000], Loss: 68.6823\n",
            "Epoch [900/1000], Loss: 68.6595\n",
            "Epoch [905/1000], Loss: 68.6370\n",
            "Epoch [910/1000], Loss: 68.6146\n",
            "Epoch [915/1000], Loss: 68.5924\n",
            "Epoch [920/1000], Loss: 68.5705\n",
            "Epoch [925/1000], Loss: 68.5487\n",
            "Epoch [930/1000], Loss: 68.5271\n",
            "Epoch [935/1000], Loss: 68.5057\n",
            "Epoch [940/1000], Loss: 68.4844\n",
            "Epoch [945/1000], Loss: 68.4634\n",
            "Epoch [950/1000], Loss: 68.4425\n",
            "Epoch [955/1000], Loss: 68.4218\n",
            "Epoch [960/1000], Loss: 68.4013\n",
            "Epoch [965/1000], Loss: 68.3809\n",
            "Epoch [970/1000], Loss: 68.3607\n",
            "Epoch [975/1000], Loss: 68.3406\n",
            "Epoch [980/1000], Loss: 68.3207\n",
            "Epoch [985/1000], Loss: 68.3010\n",
            "Epoch [990/1000], Loss: 68.2815\n",
            "Epoch [995/1000], Loss: 68.2620\n",
            "Epoch [1000/1000], Loss: 68.2428\n",
            "Epoch [5/1000], Loss: 198.1158\n",
            "Epoch [10/1000], Loss: 167.3531\n",
            "Epoch [15/1000], Loss: 152.7341\n",
            "Epoch [20/1000], Loss: 143.3453\n",
            "Epoch [25/1000], Loss: 137.2140\n",
            "Epoch [30/1000], Loss: 133.1132\n",
            "Epoch [35/1000], Loss: 130.2798\n",
            "Epoch [40/1000], Loss: 128.2390\n",
            "Epoch [45/1000], Loss: 126.6954\n",
            "Epoch [50/1000], Loss: 125.4652\n",
            "Epoch [55/1000], Loss: 124.4339\n",
            "Epoch [60/1000], Loss: 123.5304\n",
            "Epoch [65/1000], Loss: 122.7103\n",
            "Epoch [70/1000], Loss: 121.9459\n",
            "Epoch [75/1000], Loss: 121.2201\n",
            "Epoch [80/1000], Loss: 120.5218\n",
            "Epoch [85/1000], Loss: 119.8445\n",
            "Epoch [90/1000], Loss: 119.1838\n",
            "Epoch [95/1000], Loss: 118.5369\n",
            "Epoch [100/1000], Loss: 117.9023\n",
            "Epoch [105/1000], Loss: 117.2786\n",
            "Epoch [110/1000], Loss: 116.6651\n",
            "Epoch [115/1000], Loss: 116.0614\n",
            "Epoch [120/1000], Loss: 115.4670\n",
            "Epoch [125/1000], Loss: 114.8816\n",
            "Epoch [130/1000], Loss: 114.3050\n",
            "Epoch [135/1000], Loss: 113.7370\n",
            "Epoch [140/1000], Loss: 113.1775\n",
            "Epoch [145/1000], Loss: 112.6262\n",
            "Epoch [150/1000], Loss: 112.0830\n",
            "Epoch [155/1000], Loss: 111.5478\n",
            "Epoch [160/1000], Loss: 111.0204\n",
            "Epoch [165/1000], Loss: 110.5008\n",
            "Epoch [170/1000], Loss: 109.9888\n",
            "Epoch [175/1000], Loss: 109.4842\n",
            "Epoch [180/1000], Loss: 108.9870\n",
            "Epoch [185/1000], Loss: 108.4970\n",
            "Epoch [190/1000], Loss: 108.0141\n",
            "Epoch [195/1000], Loss: 107.5383\n",
            "Epoch [200/1000], Loss: 107.0693\n",
            "Epoch [205/1000], Loss: 106.6071\n",
            "Epoch [210/1000], Loss: 106.1516\n",
            "Epoch [215/1000], Loss: 105.7026\n",
            "Epoch [220/1000], Loss: 105.2601\n",
            "Epoch [225/1000], Loss: 104.8240\n",
            "Epoch [230/1000], Loss: 104.3941\n",
            "Epoch [235/1000], Loss: 103.9704\n",
            "Epoch [240/1000], Loss: 103.5527\n",
            "Epoch [245/1000], Loss: 103.1410\n",
            "Epoch [250/1000], Loss: 102.7351\n",
            "Epoch [255/1000], Loss: 102.3350\n",
            "Epoch [260/1000], Loss: 101.9406\n",
            "Epoch [265/1000], Loss: 101.5518\n",
            "Epoch [270/1000], Loss: 101.1685\n",
            "Epoch [275/1000], Loss: 100.7906\n",
            "Epoch [280/1000], Loss: 100.4181\n",
            "Epoch [285/1000], Loss: 100.0508\n",
            "Epoch [290/1000], Loss: 99.6886\n",
            "Epoch [295/1000], Loss: 99.3316\n",
            "Epoch [300/1000], Loss: 98.9795\n",
            "Epoch [305/1000], Loss: 98.6323\n",
            "Epoch [310/1000], Loss: 98.2900\n",
            "Epoch [315/1000], Loss: 97.9525\n",
            "Epoch [320/1000], Loss: 97.6197\n",
            "Epoch [325/1000], Loss: 97.2914\n",
            "Epoch [330/1000], Loss: 96.9678\n",
            "Epoch [335/1000], Loss: 96.6486\n",
            "Epoch [340/1000], Loss: 96.3338\n",
            "Epoch [345/1000], Loss: 96.0233\n",
            "Epoch [350/1000], Loss: 95.7171\n",
            "Epoch [355/1000], Loss: 95.4151\n",
            "Epoch [360/1000], Loss: 95.1173\n",
            "Epoch [365/1000], Loss: 94.8235\n",
            "Epoch [370/1000], Loss: 94.5337\n",
            "Epoch [375/1000], Loss: 94.2479\n",
            "Epoch [380/1000], Loss: 93.9659\n",
            "Epoch [385/1000], Loss: 93.6878\n",
            "Epoch [390/1000], Loss: 93.4134\n",
            "Epoch [395/1000], Loss: 93.1427\n",
            "Epoch [400/1000], Loss: 92.8757\n",
            "Epoch [405/1000], Loss: 92.6123\n",
            "Epoch [410/1000], Loss: 92.3524\n",
            "Epoch [415/1000], Loss: 92.0960\n",
            "Epoch [420/1000], Loss: 91.8430\n",
            "Epoch [425/1000], Loss: 91.5933\n",
            "Epoch [430/1000], Loss: 91.3470\n",
            "Epoch [435/1000], Loss: 91.1040\n",
            "Epoch [440/1000], Loss: 90.8642\n",
            "Epoch [445/1000], Loss: 90.6275\n",
            "Epoch [450/1000], Loss: 90.3940\n",
            "Epoch [455/1000], Loss: 90.1635\n",
            "Epoch [460/1000], Loss: 89.9361\n",
            "Epoch [465/1000], Loss: 89.7117\n",
            "Epoch [470/1000], Loss: 89.4901\n",
            "Epoch [475/1000], Loss: 89.2715\n",
            "Epoch [480/1000], Loss: 89.0557\n",
            "Epoch [485/1000], Loss: 88.8427\n",
            "Epoch [490/1000], Loss: 88.6325\n",
            "Epoch [495/1000], Loss: 88.4249\n",
            "Epoch [500/1000], Loss: 88.2201\n",
            "Epoch [505/1000], Loss: 88.0179\n",
            "Epoch [510/1000], Loss: 87.8182\n",
            "Epoch [515/1000], Loss: 87.6212\n",
            "Epoch [520/1000], Loss: 87.4266\n",
            "Epoch [525/1000], Loss: 87.2345\n",
            "Epoch [530/1000], Loss: 87.0449\n",
            "Epoch [535/1000], Loss: 86.8576\n",
            "Epoch [540/1000], Loss: 86.6728\n",
            "Epoch [545/1000], Loss: 86.4902\n",
            "Epoch [550/1000], Loss: 86.3100\n",
            "Epoch [555/1000], Loss: 86.1320\n",
            "Epoch [560/1000], Loss: 85.9562\n",
            "Epoch [565/1000], Loss: 85.7826\n",
            "Epoch [570/1000], Loss: 85.6112\n",
            "Epoch [575/1000], Loss: 85.4419\n",
            "Epoch [580/1000], Loss: 85.2747\n",
            "Epoch [585/1000], Loss: 85.1095\n",
            "Epoch [590/1000], Loss: 84.9464\n",
            "Epoch [595/1000], Loss: 84.7853\n",
            "Epoch [600/1000], Loss: 84.6262\n",
            "Epoch [605/1000], Loss: 84.4690\n",
            "Epoch [610/1000], Loss: 84.3137\n",
            "Epoch [615/1000], Loss: 84.1603\n",
            "Epoch [620/1000], Loss: 84.0088\n",
            "Epoch [625/1000], Loss: 83.8591\n",
            "Epoch [630/1000], Loss: 83.7112\n",
            "Epoch [635/1000], Loss: 83.5650\n",
            "Epoch [640/1000], Loss: 83.4207\n",
            "Epoch [645/1000], Loss: 83.2780\n",
            "Epoch [650/1000], Loss: 83.1371\n",
            "Epoch [655/1000], Loss: 82.9978\n",
            "Epoch [660/1000], Loss: 82.8601\n",
            "Epoch [665/1000], Loss: 82.7241\n",
            "Epoch [670/1000], Loss: 82.5897\n",
            "Epoch [675/1000], Loss: 82.4569\n",
            "Epoch [680/1000], Loss: 82.3256\n",
            "Epoch [685/1000], Loss: 82.1959\n",
            "Epoch [690/1000], Loss: 82.0676\n",
            "Epoch [695/1000], Loss: 81.9409\n",
            "Epoch [700/1000], Loss: 81.8156\n",
            "Epoch [705/1000], Loss: 81.6917\n",
            "Epoch [710/1000], Loss: 81.5693\n",
            "Epoch [715/1000], Loss: 81.4483\n",
            "Epoch [720/1000], Loss: 81.3287\n",
            "Epoch [725/1000], Loss: 81.2104\n",
            "Epoch [730/1000], Loss: 81.0935\n",
            "Epoch [735/1000], Loss: 80.9778\n",
            "Epoch [740/1000], Loss: 80.8635\n",
            "Epoch [745/1000], Loss: 80.7505\n",
            "Epoch [750/1000], Loss: 80.6388\n",
            "Epoch [755/1000], Loss: 80.5283\n",
            "Epoch [760/1000], Loss: 80.4190\n",
            "Epoch [765/1000], Loss: 80.3109\n",
            "Epoch [770/1000], Loss: 80.2041\n",
            "Epoch [775/1000], Loss: 80.0984\n",
            "Epoch [780/1000], Loss: 79.9939\n",
            "Epoch [785/1000], Loss: 79.8905\n",
            "Epoch [790/1000], Loss: 79.7882\n",
            "Epoch [795/1000], Loss: 79.6871\n",
            "Epoch [800/1000], Loss: 79.5871\n",
            "Epoch [805/1000], Loss: 79.4881\n",
            "Epoch [810/1000], Loss: 79.3902\n",
            "Epoch [815/1000], Loss: 79.2934\n",
            "Epoch [820/1000], Loss: 79.1976\n",
            "Epoch [825/1000], Loss: 79.1028\n",
            "Epoch [830/1000], Loss: 79.0091\n",
            "Epoch [835/1000], Loss: 78.9163\n",
            "Epoch [840/1000], Loss: 78.8245\n",
            "Epoch [845/1000], Loss: 78.7337\n",
            "Epoch [850/1000], Loss: 78.6438\n",
            "Epoch [855/1000], Loss: 78.5549\n",
            "Epoch [860/1000], Loss: 78.4669\n",
            "Epoch [865/1000], Loss: 78.3798\n",
            "Epoch [870/1000], Loss: 78.2936\n",
            "Epoch [875/1000], Loss: 78.2083\n",
            "Epoch [880/1000], Loss: 78.1239\n",
            "Epoch [885/1000], Loss: 78.0403\n",
            "Epoch [890/1000], Loss: 77.9576\n",
            "Epoch [895/1000], Loss: 77.8758\n",
            "Epoch [900/1000], Loss: 77.7947\n",
            "Epoch [905/1000], Loss: 77.7145\n",
            "Epoch [910/1000], Loss: 77.6351\n",
            "Epoch [915/1000], Loss: 77.5565\n",
            "Epoch [920/1000], Loss: 77.4787\n",
            "Epoch [925/1000], Loss: 77.4017\n",
            "Epoch [930/1000], Loss: 77.3254\n",
            "Epoch [935/1000], Loss: 77.2499\n",
            "Epoch [940/1000], Loss: 77.1751\n",
            "Epoch [945/1000], Loss: 77.1010\n",
            "Epoch [950/1000], Loss: 77.0277\n",
            "Epoch [955/1000], Loss: 76.9551\n",
            "Epoch [960/1000], Loss: 76.8832\n",
            "Epoch [965/1000], Loss: 76.8120\n",
            "Epoch [970/1000], Loss: 76.7415\n",
            "Epoch [975/1000], Loss: 76.6717\n",
            "Epoch [980/1000], Loss: 76.6025\n",
            "Epoch [985/1000], Loss: 76.5340\n",
            "Epoch [990/1000], Loss: 76.4661\n",
            "Epoch [995/1000], Loss: 76.3989\n",
            "Epoch [1000/1000], Loss: 76.3323\n",
            "final_MSE for ridge: 77.37253\n",
            "final_MSE for lasso: 59.935925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsfW_gKMuHAh"
      },
      "source": [
        "# 2. Autoencoder (50pt)\n",
        "**Autoencoder is an unsupervised neural network model for learning representations of the input. In the figure below, you can see the structure of an autoencoder network. Given the original input image, we first encode the image using ``Encoder`` to a compressed representation, and reconstruct the image by using ``Decoder`` given the compressed representation. The compressed represesntation can be used as the dimension reduced representation of the original input. In this regard, autoencoder is also known as a model for dimensionality reduction (Recall PCA was also a method for dimensionality reduction). Note that the encoder and the decoder can be any neural network model such as MLP, CNN, MLP, etc.**\n",
        "\n",
        "**For this question, you will use MNIST dataset to implement two versions of autoencoders: 1) An MLP-based autoencoder, and 2) A CNN-based autoencoder. The figure below shows an example of the MLP-based autoencoder. For the MLP-based autoencoder, you should follow the structure of the autoencoder shown in the figure below. For the CNN-based autoencoder, you are free to choose the architecture. You only need to implement ``Autoencoder_MLP`` class and ``Autoencoder_CNN`` class. Note that for ``Autoencoder_MLP``, you should flatten the original image into a vector, whereas for ``Autoencoder_CNN``, you can use the original image without any modification. After implementing these two classes, save the figures and observe the results. Write a few sentences to describe your findings. You do not need to submit the saved figure and the dataset for your final submission.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJGPYuG3uHAi"
      },
      "source": [
        "<div>\n",
        "<img src=\"figures/autoencoder.png\" width=\"700\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rgaCHRhTuHAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "6567c7d9-a47b-4950-d1fa-1b88a2580d98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5f228bf0a610>\u001b[0m in \u001b[0;36m<cell line: 111>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m#print(img.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m#img.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m#print(output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#에러 메시지 때문에 추가하였습니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-5f228bf0a610>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m## Write your answer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Make a directory \"saved_img\" if it does not exist\n",
        "#if not os.path.exists('./saved_img'):\n",
        "#    os.mkdir('./saved_img')\n",
        "\n",
        "if not os.path.exists(INPUT_DIR + '/saved_img'):\n",
        "    os.mkdir(INPUT_DIR + '/saved_img')\n",
        "\n",
        "\n",
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 1, 28, 28)\n",
        "    return x\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "#dataset = MNIST('./data', transform=img_transform)\n",
        "dataset = MNIST(INPUT_DIR+'/data', transform=img_transform)  # use Google Colab\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Autoencoder_MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder_MLP, self).__init__()\n",
        "        ## Write your answer here\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 3)\n",
        "                    )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(3, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 28*28),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        ## Write your answer here\n",
        "        x = x.cuda()\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Autoencoder_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder_CNN, self).__init__()\n",
        "        ## Write your answer here\n",
        "        self.encoder = nn.Sequential(\n",
        "                nn.Conv2d(1,5,5),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(5, 10,10),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2,2)\n",
        "                    )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "                nn.ConvTranspose2d(10,5,5),\n",
        "                nn.ReLU(),\n",
        "                nn.ConvTranspose2d(5, 1, 24),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        ## Write your answer here\n",
        "        x = x.cuda()\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Uncomment below correspondingly\n",
        "\n",
        "model = Autoencoder_MLP().cuda()\n",
        "#model = Autoencoder_CNN().cuda()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        img, _ = data\n",
        "\n",
        "        # Flatten the image for Autoencoder_MLP (You can remove this line for Autoencoder_CNN)\n",
        "        img = img.view(img.size(0), -1)\n",
        "\n",
        "        # forward pass\n",
        "        output = model(img)\n",
        "        #print(output.size())\n",
        "        img = img.to(output.device)  #에러 메시지 때문에 추가하였습니다\n",
        "        loss = criterion(output, img)\n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print log and save images\n",
        "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
        "    if epoch % 10 == 0:\n",
        "        pic = to_img(output.cpu().data)\n",
        "        save_image(pic, INPUT_DIR+'/saved_img/image_{}.png'.format(epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder_MLP 와 Autoencoder_CNN 에 의해 만들어진 이미지를 관찰한 결과 공통적으로 train을 거듭할수록 이미지 파일의 숫자가 더 선명해지는 것을 관찰할 수 있었다. 또한 두 모델을 비교했을 때 ,loss값에서나 이미지의 선명도 면에서 큰 차이는 없었다."
      ],
      "metadata": {
        "id": "t1yVQh-TzElh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Y05Ot7TzDcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}